{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = np.array([11231, 999, 123142])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = np.array([-10, 10, 24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(vec):\n",
    "    vec -= np.max(vec)\n",
    "    exp = np.exp(vec)\n",
    "    return exp / np.sum(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.71390701e-15, 8.31528028e-07, 9.99999168e-01])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment-04 基于维基百科的词向量构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本章，你将使用Gensim和维基百科获得你的第一批词向量，并且感受词向量的基本过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.kaggleusercontent.com/kf/1018109/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..JNNggcCCDcYEypvp7ZDwOA.cM9CuDpuCKo0K_ZkMFLAUvhfip0P6SRZ4LddwgTtgwz8pQy1dZeGVJWi6u81KSpAFNSi7YximVVJbPw8xsFySdWlqoUwvSER-LLIRfmlpsCvtDt90NaLYT2FHlwl0tfF-1MKtiFsWlGQ8LGo40hL3ccBSwMZy214kGJf9bNkW_g.kZbF5sgN5qha3zhjilfSDg/__results___files/__results___9_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-01: Download Wikipedia Chinese Corpus: https://dumps.wikimedia.org/zhwiki/20190720/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一步：使用维基百科下载中文语料库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``downloaded file: zhwiki-20190720-pages-articles-multistream.xml.bz2``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-02: Using https://github.com/attardi/wikiextractor to extract the wikipedia corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二步：使用python wikipedia extractor抽取维基百科的内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "bzip2 -d zhwiki-20190720-pages-articles-multistream.xml.bz2 # decompress\n",
    "./wikiextractor/WikiExtractor.py zhwiki-20190720-pages-articles-multistream.xml # extract\n",
    "./text # extracted file location\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-03: Using gensim get word vectors: \n",
    "\n",
    "Reference: \n",
    "\n",
    "+ https://radimrehurek.com/gensim/models/word2vec.html\n",
    "+ https://www.kaggle.com/jeffd23/visualizing-word-vectors-with-t-sne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第三步：参考Gensim的文档和Kaggle的参考文档，获得词向量。 注意，你要使用Jieba分词把维基百科的内容切分成一个一个单词，然后存进新的文件中。然后，你需要用Gensim的LineSentence这个类进行文件的读取。\n",
    "\n",
    "在训练成词向量Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jieba in /home/weiliang/anaconda3/lib/python3.7/site-packages (0.39)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/weiliang/anaconda3/lib/python3.7/site-packages (4.7.1)\n",
      "Requirement already satisfied: soupsieve>=1.2 in /home/weiliang/anaconda3/lib/python3.7/site-packages (from beautifulsoup4) (1.8)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install jieba beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing files under: ./text/AL\n",
      "Finished processing files under: ./text/AM\n",
      "Finished processing files under: ./text/AI\n",
      "Finished processing files under: ./text/AD\n",
      "Finished processing files under: ./text/AK\n",
      "Finished processing files under: ./text/AA\n",
      "Finished processing files under: ./text/AF\n",
      "Finished processing files under: ./text/AN\n",
      "Finished processing files under: ./text/AH\n",
      "Finished processing files under: ./text/AE\n",
      "Finished processing files under: ./text/AG\n",
      "Finished processing files under: ./text/AJ\n",
      "Finished processing files under: ./text/AC\n",
      "Finished processing files under: ./text/AB\n",
      "Finished processing files under: ./text\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import jieba\n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "corp_file = os.path.abspath(\"./wiki_processed.corp\")\n",
    "corps =  open(corp_file, 'a+')\n",
    "\n",
    "\n",
    "def clean(string):\n",
    "    return ''.join(re.findall(r'\\w+', string))\n",
    "\n",
    "def cut(string):\n",
    "    return list(jieba.cut(string))\n",
    "\n",
    "def preprocess_wiki_data(root_dir):\n",
    "    dirs = os.listdir(root_dir)\n",
    "    for d in dirs:\n",
    "        temp = root_dir + '/' + d\n",
    "        if os.path.isdir(temp):\n",
    "            preprocess_wiki_data(temp)\n",
    "        elif os.path.exists(temp):\n",
    "            corps_str = ''\n",
    "            with open(temp, 'r') as f:\n",
    "               \n",
    "                soup = BeautifulSoup(f)\n",
    "                docs = soup.select('doc')\n",
    "                for doc in docs:\n",
    "                    content = str(doc.string)\n",
    "                    corp = cut(clean(content))\n",
    "                    corp_str = ' '.join(corp)\n",
    "                    corps.write(corp_str + \"\\n\")\n",
    "        else:\n",
    "            return\n",
    "    print(\"Finished processing files under: {}\".format(root_dir))\n",
    "            \n",
    "preprocess_wiki_data('./text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-04: Using some words to test your preformance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.douban.com/simple\n",
      "Collecting gensim\n",
      "\u001b[?25l  Downloading https://pypi.doubanio.com/packages/c8/3a/32a1edf4f335eba0873021a7ddb3230f05dedd2b5450960118b402ca0771/gensim-3.8.0-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (24.7MB)\n",
      "\u001b[K    100% |████████████████████████████████| 24.7MB 1.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.11.3 in /Users/weiliang/anaconda3/lib/python3.7/site-packages (from gensim) (1.15.4)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /Users/weiliang/anaconda3/lib/python3.7/site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /Users/weiliang/anaconda3/lib/python3.7/site-packages (from gensim) (1.1.0)\n",
      "Collecting smart-open>=1.7.0 (from gensim)\n",
      "\u001b[?25l  Downloading https://pypi.doubanio.com/packages/37/c0/25d19badc495428dec6a4bf7782de617ee0246a9211af75b302a2681dea7/smart_open-1.8.4.tar.gz (63kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 433kB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: boto>=2.32 in /Users/weiliang/anaconda3/lib/python3.7/site-packages (from smart-open>=1.7.0->gensim) (2.49.0)\n",
      "Requirement already satisfied, skipping upgrade: requests in /Users/weiliang/anaconda3/lib/python3.7/site-packages (from smart-open>=1.7.0->gensim) (2.21.0)\n",
      "Collecting boto3 (from smart-open>=1.7.0->gensim)\n",
      "\u001b[?25l  Downloading https://pypi.doubanio.com/packages/78/a4/1448a8c827d19ab49b98fc6767bb2cf689386ef8102502cbc956d4faa638/boto3-1.9.203-py2.py3-none-any.whl (128kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 436kB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /Users/weiliang/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.7.0->gensim) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /Users/weiliang/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.7.0->gensim) (2018.11.29)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /Users/weiliang/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.7.0->gensim) (1.24.1)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /Users/weiliang/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.7.0->gensim) (2.8)\n",
      "Collecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.7.0->gensim)\n",
      "  Downloading https://pypi.doubanio.com/packages/83/94/7179c3832a6d45b266ddb2aac329e101367fbdb11f425f13771d27f225bb/jmespath-0.9.4-py2.py3-none-any.whl\n",
      "Collecting s3transfer<0.3.0,>=0.2.0 (from boto3->smart-open>=1.7.0->gensim)\n",
      "\u001b[?25l  Downloading https://pypi.doubanio.com/packages/16/8a/1fc3dba0c4923c2a76e1ff0d52b305c44606da63f718d14d3231e21c51b0/s3transfer-0.2.1-py2.py3-none-any.whl (70kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 174kB/s ta 0:00:011\n",
      "\u001b[?25hCollecting botocore<1.13.0,>=1.12.203 (from boto3->smart-open>=1.7.0->gensim)\n",
      "\u001b[?25l  Downloading https://pypi.doubanio.com/packages/78/ab/168a3ec74ab3f6e1aa066ef3c77082749017345cbb2ccb1477ed2f21d19b/botocore-1.12.203-py2.py3-none-any.whl (5.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 5.6MB 1.0MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: docutils<0.15,>=0.10 in /Users/weiliang/anaconda3/lib/python3.7/site-packages (from botocore<1.13.0,>=1.12.203->boto3->smart-open>=1.7.0->gensim) (0.14)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /Users/weiliang/anaconda3/lib/python3.7/site-packages (from botocore<1.13.0,>=1.12.203->boto3->smart-open>=1.7.0->gensim) (2.7.5)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Running setup.py bdist_wheel for smart-open ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/weiliang/Library/Caches/pip/wheels/d6/99/34/8ef455ebe1eac3de61bcf4793a3c1ef68431ca3ebeec804488\n",
      "Successfully built smart-open\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3, smart-open, gensim\n",
      "Successfully installed boto3-1.9.203 botocore-1.12.203 gensim-3.8.0 jmespath-0.9.4 s3transfer-0.2.1 smart-open-1.8.4\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "model_file = \"zh_word2vec.model\"\n",
    "if os.path.exists(model_file):\n",
    "    model = Word2Vec.load(model_file)\n",
    "else:\n",
    "    sentences = LineSentence('./wiki_processed.corp')\n",
    "    model = Word2Vec(sentences, size=100, window=5, min_count=100, workers=4)\n",
    "    model.save(model_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('韭菜', 0.8098064661026001),\n",
       " ('地瓜', 0.8060703873634338),\n",
       " ('芹菜', 0.8038861751556396),\n",
       " ('冬瓜', 0.8020275235176086),\n",
       " ('柿子', 0.7961002588272095),\n",
       " ('瓜果', 0.7946608662605286),\n",
       " ('茄子', 0.7857564091682434),\n",
       " ('油桐', 0.7835367321968079),\n",
       " ('蔥', 0.7817407846450806),\n",
       " ('白菜', 0.7795143127441406)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(['西瓜'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('头痛', 0.9274026155471802),\n",
       " ('呕吐', 0.9161430597305298),\n",
       " ('头晕', 0.8861633539199829),\n",
       " ('呼吸困难', 0.8693729639053345),\n",
       " ('腹泻', 0.8650820255279541),\n",
       " ('心悸', 0.8102750778198242),\n",
       " ('眩晕', 0.8072993159294128),\n",
       " ('焦虑', 0.8028079867362976),\n",
       " ('狂躁', 0.8024279475212097),\n",
       " ('抑郁', 0.802391767501831)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(['恶心'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第四步，测试同义词，找几个单词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-05: Using visualization tools: https://www.kaggle.com/jeffd23/visualizing-word-vectors-with-t-sne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第五步：使用Kaggle给出的T-SEN进行词向量的可视化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def tsne_plot(model):\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    for word in model.wv.vocab:\n",
    "        tokens.append(model[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(16, 16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/weiliang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "tsne_plot(model) # This function takes too much time and never draw successfully on my local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
